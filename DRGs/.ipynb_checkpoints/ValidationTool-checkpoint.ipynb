{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ak055384\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import  shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "#From Scikit Learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection  import train_test_split, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "import time\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "from functools import partial\n",
    "\n",
    "logPath = \"./tb_logs\"\n",
    "features = 2500\n",
    "n_class = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "interestingColumns = ['MDC', 'ADMTNG_ICD9_DGNS_CD','ICD9_DGNS_CD_1', 'ICD9_DGNS_CD_2',\n",
    "    'ICD9_DGNS_CD_3', 'ICD9_DGNS_CD_4', 'ICD9_DGNS_CD_5', 'ICD9_DGNS_CD_6',\n",
    "    'ICD9_DGNS_CD_7', 'ICD9_DGNS_CD_8', 'ICD9_DGNS_CD_9', 'ICD9_DGNS_CD_10',\n",
    "    'ICD9_PRCDR_CD_1', 'ICD9_PRCDR_CD_2', 'ICD9_PRCDR_CD_3',\n",
    "    'ICD9_PRCDR_CD_4', 'ICD9_PRCDR_CD_5', 'ICD9_PRCDR_CD_6', 'BENE_SEX_IDENT_CD', 'BENE_RACE_CD', 'BENE_BIRTH_DT','SP_STATE_CODE']\n",
    "dgnColumns = ['ADMTNG_ICD9_DGNS_CD', 'ICD9_DGNS_CD_1', 'ICD9_DGNS_CD_2',\n",
    "    'ICD9_DGNS_CD_3', 'ICD9_DGNS_CD_4', 'ICD9_DGNS_CD_5', 'ICD9_DGNS_CD_6',\n",
    "    'ICD9_DGNS_CD_7', 'ICD9_DGNS_CD_8', 'ICD9_DGNS_CD_9', 'ICD9_DGNS_CD_10']\n",
    "prcdrColumns = [ 'ICD9_PRCDR_CD_1', 'ICD9_PRCDR_CD_2', 'ICD9_PRCDR_CD_3',\n",
    "    'ICD9_PRCDR_CD_4', 'ICD9_PRCDR_CD_5', 'ICD9_PRCDR_CD_6']\n",
    "demoColumns = ['BENE_SEX_IDENT_CD', 'BENE_RACE_CD', 'BENE_BIRTH_DT','SP_STATE_CODE']\n",
    "\n",
    "def testSample(data):\n",
    "\n",
    "    finalcolumns = dgnColumns + prcdrColumns + demoColumns\n",
    "    joinClaims = pd.DataFrame(data, columns = finalcolumns)\n",
    "\n",
    "    claims_subset_df = joinClaims[dgnColumns + prcdrColumns]\n",
    "    claims_subset_df = claims_subset_df.reset_index(drop=True)\n",
    "    # print(claims_subset_df.shape)\n",
    "    demo_subset_df = joinClaims[demoColumns]\n",
    "    # print(demo_subset_df.shape)\n",
    "    demo_subset_df = demo_subset_df.reset_index(drop=True)\n",
    "    # print(demo_subset_df.shape)\n",
    "    demo_subset_df.fillna(0)\n",
    "    combineClaims_subset = pd.concat([claims_subset_df, demo_subset_df], axis=1)\n",
    "    # print(combineClaims_subset.shape)\n",
    "    claims_subset_df.fillna(0)\n",
    "    # print(claims_subset_df.dtypes)\n",
    "    demo_subset_df = demo_subset_df.astype('int64', copy=False)\n",
    "\n",
    "    start_time = time.clock()\n",
    "    # skip first column the target variablee\n",
    "    j=1\n",
    "    encoder_file = open(\"cat_encoder_mdc_\" + str(j) + \".pkl\", \"rb\")\n",
    "    cat = pickle.load(encoder_file)\n",
    "    train_df = cat.transform(claims_subset_df['ADMTNG_ICD9_DGNS_CD'].values.reshape(-1,1))\n",
    "    encoder_file.close()\n",
    "    train_df_c = pd.DataFrame(train_df.toarray(), columns=cat.categories_)\n",
    "\n",
    "    for col in interestingColumns[2:18]:   \n",
    "        j=j+1 \n",
    "        encoder_file = open(\"cat_encoder_mdc_\" + str(j) + \".pkl\", \"rb\")\n",
    "        cat = pickle.load(encoder_file)\n",
    "        train_df = cat.transform(claims_subset_df[col].values.reshape(-1,1))\n",
    "        encoder_file.close()\n",
    "        train_df_c = train_df_c.append(pd.DataFrame(train_df.toarray(), columns=cat.categories_))\n",
    "        train_df_c = train_df_c.fillna(0)\n",
    "        train_df_c = train_df_c.groupby(train_df_c.index).sum()\n",
    "        # print(train_df_c.shape)\n",
    "        # print(\"Time to run\", time.clock() - start_time, \"seconds\")\n",
    "\n",
    "    # print(train_df_c.shape)\n",
    "    train_df_new = pd.concat([train_df_c,demo_subset_df ], axis=1)\n",
    "    # print(train_df_new.shape) \n",
    "    # print(train_df_new.dtypes)\n",
    "    pca_file = open(\"pca_mdc_1.pkl\", \"rb\")\n",
    "    pca = pickle.load(pca_file)    \n",
    "    x = train_df_new.values.tolist()\n",
    "    X = pca.transform(x)\n",
    "    pca_file.close()\n",
    "    return X\n",
    "\n",
    "def getPredictedMDC(inputs):\n",
    "    dataReshaped = np.array(inputs).reshape(1,21)\n",
    "    testX = testSample(dataReshaped)\n",
    "    feature_size = testX.shape[1]\n",
    "    print(feature_size)\n",
    "\n",
    "    with tf.name_scope(\"input_parameters\"):\n",
    "        feature_size=2500\n",
    "        n_class = 25\n",
    "\n",
    "    with tf.name_scope(\"Claims_Data_Input\"):\n",
    "        x = tf.placeholder(tf.float32,[None,feature_size], name =\"diag_proc_demo_data\")\n",
    "\n",
    "    with tf.name_scope('weights'):\n",
    "        def weight_variable(feature_size, n_class, name=None):\n",
    "            initial = tf.truncated_normal([feature_size, n_class])\n",
    "            weights = tf.Variable(initial, name=name)\n",
    "            return weights\n",
    "\n",
    "    with tf.name_scope('biases'):\n",
    "        def bias_variable(n_class, name=None):\n",
    "            initial = tf.truncated_normal([n_class])\n",
    "            biases = tf.Variable(initial, name=name)\n",
    "            return biases\n",
    "\n",
    "    with tf.name_scope('mdc_neural_network'):\n",
    "        weights = weight_variable(feature_size, n_class, name='Weight')\n",
    "        biases = bias_variable(n_class, name='biases')\n",
    "        logits = tf.matmul(x, weights) + biases\n",
    "\n",
    "    with tf.name_scope(\"predicted_mdc\"):\n",
    "        y_conv = tf.nn.softmax(logits=logits)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, \"/tmp/model_mdc.ckpt\")\n",
    "        prediction = tf.argmax(y_conv, axis = 1)\n",
    "        return prediction.eval(feed_dict={x: testX}, session=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDRG(predY_encoded):\n",
    "    label_file = open(\"label_1.pkl\", \"rb\")\n",
    "    encoder = pickle.load(label_file)\n",
    "    predY = encoder.inverse_transform(predY_encoded)\n",
    "    label_file.close()\n",
    "    print('Predicted MDC ', predY)\n",
    "    if predY[0] == 'MDC-05':\n",
    "        finalcolumns = dgnColumns + prcdrColumns + demoColumns\n",
    "        joinClaims = pd.DataFrame(dataReshaped, columns = finalcolumns)\n",
    "        claims_subset_df = joinClaims[dgnColumns + prcdrColumns]\n",
    "        claims_subset_df = claims_subset_df.reset_index(drop=True)\n",
    "        # print(claims_subset_df.shape)\n",
    "        demo_subset_df = joinClaims[demoColumns]\n",
    "        # print(demo_subset_df.shape)\n",
    "        demo_subset_df = demo_subset_df.reset_index(drop=True)\n",
    "        # print(demo_subset_df.shape)\n",
    "        demo_subset_df.fillna(0)\n",
    "        combineClaims_subset = pd.concat([claims_subset_df, demo_subset_df], axis=1)\n",
    "        # print(combineClaims_subset.shape)\n",
    "        claims_subset_df.fillna(0.0)\n",
    "        # print(claims_subset_df.dtypes)\n",
    "        demo_subset_df = demo_subset_df.astype('int64', copy=False)\n",
    "\n",
    "        start_time = time.clock()\n",
    "        # skip first column the target variablee\n",
    "        j=1\n",
    "        encoder_file = open(\"cat_encoder_mdc5_\" + str(j) + \".pkl\", \"rb\")\n",
    "        cat = pickle.load(encoder_file)\n",
    "        train_df = cat.transform(claims_subset_df['ADMTNG_ICD9_DGNS_CD'].values.reshape(-1,1))\n",
    "\n",
    "        encoder_file.close()\n",
    "        train_df_c = pd.DataFrame(train_df.toarray(), columns=cat.categories_)\n",
    "\n",
    "        for col in interestingColumns[2:18]:   \n",
    "            j=j+1 \n",
    "            encoder_file = open(\"cat_encoder_mdc5_\" + str(j) + \".pkl\", \"rb\")\n",
    "            cat = pickle.load(encoder_file)\n",
    "            train_df = cat.transform(claims_subset_df[col].values.reshape(-1,1))\n",
    "            encoder_file.close()\n",
    "            train_df_c = train_df_c.append(pd.DataFrame(train_df.toarray(), columns=cat.categories_))\n",
    "            train_df_c = train_df_c.fillna(0)\n",
    "            train_df_c = train_df_c.groupbyDRG  (train_df_c.index).sum()\n",
    "            # print(train_df_c.shape)\n",
    "            # print(\"Time to run\", time.clock() - start_time, \"seconds\")\n",
    "        # print(train_df_c.shape)\n",
    "        train_df_new = pd.concat([train_df_c,demo_subset_df ], axis=1)\n",
    "        # print(train_df_new.shape)\n",
    "        X_test = train_df_new\n",
    "        clf_dt_file = open(\"clf_dt_decision_mdc5.pkl\", \"rb\")\n",
    "        clf_dt = pickle.load(clf_dt_file)\n",
    "        dt_predicted = clf_dt.predict(X_test)\n",
    "        return dt_predicted\n",
    "\n",
    "    elif predY[0] == 'MDC-04':\n",
    "        finalcolumns = dgnColumns + prcdrColumns + demoColumns\n",
    "        joinClaims = pd.DataFrame(dataReshaped, columns = finalcolumns)\n",
    "        claims_subset_df = joinClaims[dgnColumns + prcdrColumns]\n",
    "        claims_subset_df = claims_subset_df.reset_index(drop=True)\n",
    "        #print(claims_subset_df.shape)\n",
    "        demo_subset_df = joinClaims[demoColumns]\n",
    "        #print(demo_subset_df.shape)\n",
    "        demo_subset_df = demo_subset_df.reset_index(drop=True)\n",
    "        #print(demo_subset_df.shape)\n",
    "        demo_subset_df.fillna(0)\n",
    "        combineClaims_subset = pd.concat([claims_subset_df, demo_subset_df], axis=1)\n",
    "        #print(combineClaims_subset.shape)\n",
    "        claims_subset_df.fillna(0.0)\n",
    "        # print(claims_subset_df.dtypes)\n",
    "        demo_subset_df = demo_subset_df.astype('int64', copy=False)\n",
    "\n",
    "        start_time = time.clock()\n",
    "        # skip first column the target variablee\n",
    "        j=1\n",
    "        encoder_file = open(\"cat_encoder_mdc4_\" + str(j) + \".pkl\", \"rb\")\n",
    "        cat = pickle.load(encoder_file)\n",
    "        train_df = cat.transform(claims_subset_df['ADMTNG_ICD9_DGNS_CD'].values.reshape(-1,1))\n",
    "\n",
    "        encoder_file.close()\n",
    "        train_df_c = pd.DataFrame(train_df.toarray(), columns=cat.categories_)\n",
    "\n",
    "        for col in interestingColumns[2:18]:   \n",
    "            j=j+1 \n",
    "            encoder_file = open(\"cat_encoder_mdc4_\" + str(j) + \".pkl\", \"rb\")\n",
    "            cat = pickle.load(encoder_file)\n",
    "            train_df = cat.transform(claims_subset_df[col].values.reshape(-1,1))\n",
    "            encoder_file.close()\n",
    "            train_df_c = train_df_c.append(pd.DataFrame(train_df.toarray(), columns=cat.categories_))\n",
    "            train_df_c = train_df_c.fillna(0)\n",
    "            train_df_c = train_df_c.groupby(train_df_c.index).sum()\n",
    "            # print(train_df_c.shape)\n",
    "            # print(\"Time to run\", time.clock() - start_time, \"seconds\")\n",
    "        # print(train_df_c.shape)\n",
    "        train_df_new = pd.concat([train_df_c,demo_subset_df ], axis=1)\n",
    "        # print(train_df_new.shape)\n",
    "        X_test = train_df_new\n",
    "        clf_dt_file = open(\"clf_dt_decision_mdc4.pkl\", \"rb\")\n",
    "        clf_dt = pickle.load(clf_dt_file)\n",
    "        dt_predicted = clf_dt.predict(X_test)\n",
    "        return dt_predicted\n",
    "\n",
    "    elif predY[0] == 'MDC-03':\n",
    "        finalcolumns = dgnColumns + prcdrColumns + demoColumns\n",
    "        joinClaims = pd.DataFrame(dataReshaped, columns = finalcolumns)\n",
    "        claims_subset_df = joinClaims[dgnColumns + prcdrColumns]\n",
    "        claims_subset_df = claims_subset_df.reset_index(drop=True)\n",
    "        print(claims_subset_df.shape)\n",
    "        demo_subset_df = joinClaims[demoColumns]\n",
    "        print(demo_subset_df.shape)\n",
    "        demo_subset_df = demo_subset_df.reset_index(drop=True)\n",
    "        print(demo_subset_df.shape)\n",
    "        demo_subset_df.fillna(0)\n",
    "        combineClaims_subset = pd.concat([claims_subset_df, demo_subset_df], axis=1)\n",
    "        print(combineClaims_subset.shape)\n",
    "        claims_subset_df.fillna(0.0)\n",
    "        # print(claims_subset_df.dtypes)\n",
    "        demo_subset_df = demo_subset_df.astype('int64', copy=False)\n",
    "\n",
    "        start_time = time.clock()\n",
    "        # skip first column the target variablee\n",
    "        j=1\n",
    "        encoder_file = open(\"cat_encoder_mdc3_\" + str(j) + \".pkl\", \"rb\")\n",
    "        cat = pickle.load(encoder_file)\n",
    "        train_df = cat.transform(claims_subset_df['ADMTNG_ICD9_DGNS_CD'].values.reshape(-1,1))\n",
    "\n",
    "        encoder_file.close()\n",
    "        train_df_c = pd.DataFrame(train_df.toarray(), columns=cat.categories_)\n",
    "\n",
    "        for col in interestingColumns[2:18]:   \n",
    "            j=j+1 \n",
    "            encoder_file = open(\"cat_encoder_mdc3_\" + str(j) + \".pkl\", \"rb\")\n",
    "            cat = pickle.load(encoder_file)\n",
    "            train_df = cat.transform(claims_subset_df[col].values.reshape(-1,1))\n",
    "            encoder_file.close()\n",
    "            train_df_c = train_df_c.append(pd.DataFrame(train_df.toarray(), columns=cat.categories_))\n",
    "            train_df_c = train_df_c.fillna(0)\n",
    "            train_df_c = train_df_c.groupby(train_df_c.index).sum()\n",
    "            # print(train_df_c.shape)\n",
    "            # print(\"Time to run\", time.clock() - start_time, \"seconds\")\n",
    "        print(train_df_c.shape)\n",
    "        train_df_new = pd.concat([train_df_c,demo_subset_df ], axis=1)\n",
    "        print(train_df_new.shape)\n",
    "        X_test = train_df_new\n",
    "        clf_dt_file = open(\"clf_dt_decision_mdc3.pkl\", \"rb\")\n",
    "        clf_dt = pickle.load(clf_dt_file)\n",
    "        dt_predicted = clf_dt.predict(X_test)\n",
    "        return dt_predicted\n",
    "\n",
    "    elif predY[0] == 'MDC-01':\n",
    "        finalcolumns = dgnColumns + prcdrColumns + demoColumns\n",
    "        joinClaims = pd.DataFrame(dataReshaped, columns = finalcolumns)\n",
    "        claims_subset_df = joinClaims[dgnColumns + prcdrColumns]\n",
    "        claims_subset_df = claims_subset_df.reset_index(drop=True)\n",
    "        print(claims_subset_df.shape)\n",
    "        demo_subset_df = joinClaims[demoColumns]\n",
    "        print(demo_subset_df.shape)\n",
    "        demo_subset_df = demo_subset_df.reset_index(drop=True)\n",
    "        print(demo_subset_df.shape)\n",
    "        demo_subset_df.fillna(0)\n",
    "        combineClaims_subset = pd.concat([claims_subset_df, demo_subset_df], axis=1)\n",
    "        print(combineClaims_subset.shape)\n",
    "        claims_subset_df.fillna(0.0)\n",
    "        # print(claims_subset_df.dtypes)\n",
    "        demo_subset_df = demo_subset_df.astype('int64', copy=False)\n",
    "\n",
    "        start_time = time.clock()\n",
    "        # skip first column the target variablee\n",
    "        j=1\n",
    "        encoder_file = open(\"cat_encoder_mdc1_\" + str(j) + \".pkl\", \"rb\")\n",
    "        cat = pickle.load(encoder_file)\n",
    "        train_df = cat.transform(claims_subset_df['ADMTNG_ICD9_DGNS_CD'].values.reshape(-1,1))\n",
    "\n",
    "        encoder_file.close()\n",
    "        train_df_c = pd.DataFrame(train_df.toarray(), columns=cat.categories_)\n",
    "\n",
    "        for col in interestingColumns[2:18]:   \n",
    "            j=j+1 \n",
    "            encoder_file = open(\"cat_encoder_mdc1_\" + str(j) + \".pkl\", \"rb\")\n",
    "            cat = pickle.load(encoder_file)\n",
    "            train_df = cat.transform(claims_subset_df[col].values.reshape(-1,1))\n",
    "            encoder_file.close()\n",
    "            train_df_c = train_df_c.append(pd.DataFrame(train_df.toarray(), columns=cat.categories_))\n",
    "            train_df_c = train_df_c.fillna(0)\n",
    "            train_df_c = train_df_c.groupby(train_df_c.index).sum()\n",
    "            # print(train_df_c.shape)\n",
    "            # print(\"Time to run\", time.clock() - start_time, \"seconds\")\n",
    "        print(train_df_c.shape)\n",
    "        train_df_new = pd.concat([train_df_c,demo_subset_df ], axis=1)\n",
    "        print(train_df_new.shape)\n",
    "        X_test = train_df_new\n",
    "        clf_dt_file = open(\"clf_dt_decision_mdc1.pkl\", \"rb\")\n",
    "        clf_dt = pickle.load(clf_dt_file)\n",
    "        dt_predicted = clf_dt.predict(X_test)\n",
    "        return dt_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
